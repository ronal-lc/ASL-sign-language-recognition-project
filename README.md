# ASL Alphabet Recognizer

## Purpose/Description

This project implements a real-time American Sign Language (ASL) alphabet recognizer using computer vision and machine learning. It captures hand gestures via a webcam, processes these gestures to extract key landmarks, and uses a trained neural network to classify them into corresponding ASL alphabet letters (A-Z and a "STOP" gesture).

Key features include:
*   **Image Collection:** A script to gather image data for each ASL letter.
*   **Dataset Creation:** Processing of collected images to extract hand landmarks and create a structured dataset.
*   **Model Training:** Training a neural network for gesture classification.
*   **GUI Application:** A user-friendly interface for:
    *   Real-time ASL letter recognition using a webcam.
    *   A "Practice" mode to help users learn ASL letters by showing an image of a letter and verifying their attempt.
*   **Automated Training Pipeline:** A script to run the data preparation and model training steps in sequence.
*   **Customizable UI:** Theme selection (Light/Dark) and font scaling for better accessibility.
*   **Comprehensive Logging:** Detailed logs for monitoring and debugging.

## Project Structure

The project is organized into several Python scripts and data files:

*   `asl_alphabet.py`: The main GUI application built with PySide6 for real-time ASL recognition and the practice mode.
*   `collector_images.py`: Script for collecting image data for each ASL letter. It prompts the user to capture images for letters A-Z.
*   `create_dataset.py`: Processes the images collected by `collector_images.py`. It uses MediaPipe to extract hand landmarks, performs data augmentation (flipping images), and saves the processed data into `data_signs.pickle`.
*   `data_classify.py`: Trains the neural network model using the `data_signs.pickle` dataset. The trained model is saved as `model.keras`, and training metrics (accuracy, loss) are output to `training_metrics.json`.
*   `run_training_pipeline.py`: Automates the workflow by running `create_dataset.py` and then `data_classify.py`. It also logs training metrics from `training_metrics.json` into `training_log.csv`.
*   `utils.py`: Contains utility functions, primarily for setting up centralized logging across all scripts.
*   `requirements.txt`: Lists all Python dependencies required to run the project.
*   `data/`: Directory where images collected by `collector_images.py` are stored (organized into subdirectories for each letter A-Z). This directory is created automatically if it doesn't exist.
*   `alphabet_examples/`: Contains example images (A.jpg, B.jpg, etc.) used in the "Practice" tab of the GUI application.
*   `model.keras`: The trained Keras neural network model file. Generated by `data_classify.py`.
*   `data_signs.pickle`: The processed dataset containing hand landmarks and corresponding labels. Generated by `create_dataset.py`.
*   `training_metrics.json`: A temporary file created by `data_classify.py` holding the accuracy and loss of the last training session. It's consumed by `run_training_pipeline.py`.
*   `training_log.csv`: A CSV file where `run_training_pipeline.py` logs a history of training metrics (timestamp, accuracy, loss) for each run.
*   `*.log`: Log files (e.g., `asl_alphabet_pyside.log`, `collector_images.log`, etc.) generated by each script, containing detailed operational information and error messages.

## Installation

1.  **Clone the Repository:**
    ```bash
    git clone <repository_url>
    cd asl-alphabet-recognizer 
    ```
    (Replace `<repository_url>` with the actual URL of the repository.)

2.  **Set up a Python Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    Ensure you have Python 3.9 or newer installed. Then, install the required packages:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

Make sure you have a webcam connected to your computer before running the recognition application.

### 1. Data Collection
This step is necessary if you want to train the model on your own hand gestures or expand the dataset.
*   **Command:**
    ```bash
    python collector_images.py
    ```
*   **Explanation:** Follow the on-screen prompts. You will be asked to capture a set number of images (default is 100) for each ASL letter from A to Z. Press 'Q' to start capturing for the displayed letter. Images will be saved in the `data/` directory, organized into subfolders for each letter.

### 2. Dataset Creation
After collecting images, process them to create the landmark dataset.
*   **Command:**
    ```bash
    python create_dataset.py
    ```
*   **Explanation:** This script reads images from the `data/` directory, uses MediaPipe to extract hand landmarks for each image, performs data augmentation by horizontally flipping images, and saves the processed landmarks and labels into `data_signs.pickle`. This script uses multiprocessing for faster processing and has a checkpointing mechanism, so it can be resumed if interrupted.

### 3. Model Training
Train the neural network model using the generated dataset.
*   **Command:**
    ```bash
    python data_classify.py
    ```
*   **Explanation:** This script loads data from `data_signs.pickle`, splits it into training and testing sets, defines and compiles a Keras neural network, trains the model, and evaluates its performance. The trained model is saved as `model.keras`. Training metrics (accuracy and loss) are also saved to `training_metrics.json`.

### 4. Automated Training Pipeline
To run dataset creation and model training sequentially.
*   **Command:**
    ```bash
    python run_training_pipeline.py
    ```
*   **Explanation:** This script automates the execution of `create_dataset.py` followed by `data_classify.py`. After `data_classify.py` completes, the pipeline script reads the metrics from `training_metrics.json` and appends them along with a timestamp to `training_log.csv`. The temporary `training_metrics.json` is then deleted.

### 5. Running the GUI Application
To use the real-time ASL recognition and practice features.
*   **Command:**
    ```bash
    python asl_alphabet.py
    ```
*   **Explanation:** This launches the main PySide6 GUI application.
    *   **Recognition Tab:** Provides a live webcam feed where you can make ASL hand signs. The recognized letter and confidence score will be displayed.
    *   **Practice Tab:** Allows you to select or be shown an ASL letter image. You can then try to replicate the sign, and the application will attempt to recognize your gesture.
    *   **Settings:** Theme (Light/Dark) and font size can be adjusted from the settings panel at the bottom. Preferences are saved and loaded.

## Contributing

Contributions are welcome! If you have suggestions for improvements, new features, or find any bugs, please feel free to:
1.  Open an issue on the project's issue tracker to discuss the change.
2.  Fork the repository, make your changes, and submit a pull request.

Please ensure your code follows the existing style and includes relevant documentation or comments.

## License

This project is currently unlicensed.
